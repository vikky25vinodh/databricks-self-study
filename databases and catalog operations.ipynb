{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e118cf2f-27f3-455b-a3c2-7f7eff2164e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###create database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5b3d320-8243-45ee-b78a-91e43b61f08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create database if not exists my_db;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c20cddfa-3927-42e7-af12-394d8b3a91e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###switching to a newly created database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93a3a3e5-dfc8-4d3b-96a6-92d7e8ab7a44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use database my_db;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "123dff6a-eb4b-4a97-a2ab-d5ab1701d6ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###create a table within the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a401793-c712-4377-82f2-a790792af088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table if exists my_test_table;\n",
    "create table my_test_table(\n",
    "  customer_id int,\n",
    "  customer_name string\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "975289ec-150a-480e-ba7b-5251dc48a153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###insert data into the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d6b59f-3d56-40cd-b0e3-5489459f9417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into my_test_table values\n",
    "(0001,\"vikky\"),(0002,\"sam\");\n",
    "SELECT * FROM my_test_table;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3643273c-7661-4761-922e-e551d645888f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The previous cells demonstrate how to insert data into a table using SQL and then query it in Databricks.\n",
    "# Here is a more detailed breakdown and additional operations you can perform on Spark tables and DataFrames.\n",
    "\n",
    "# 1. Create a table (if not exists)\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS my_test_table (\n",
    "  id INT,\n",
    "  name STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# 2. Insert data into the table\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO my_test_table VALUES\n",
    "  (1, 'vikky'),\n",
    "  (2, 'sam')\n",
    "\"\"\")\n",
    "\n",
    "# 3. Query the table using SQL\n",
    "df_sql = spark.sql(\"SELECT * FROM my_test_table\")\n",
    "display(df_sql)\n",
    "\n",
    "# 4. Query the table using DataFrame API\n",
    "df = spark.table(\"my_test_table\")\n",
    "display(df)\n",
    "\n",
    "# 5. Filter rows where name is 'vikky'\n",
    "filtered_df = df.filter(df.name == 'vikky')\n",
    "display(filtered_df)\n",
    "\n",
    "# 6. Add a new column (e.g., uppercase name)\n",
    "from pyspark.sql.functions import upper\n",
    "df_with_upper = df.withColumn(\"name_upper\", upper(df.name))\n",
    "display(df_with_upper)\n",
    "\n",
    "# 7. Count the number of rows\n",
    "row_count = df.count()\n",
    "print(f\"Row count: {row_count}\")\n",
    "\n",
    "# 8. Drop the table (cleanup)\n",
    "spark.sql(\"DROP TABLE IF EXISTS my_test_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c9682d-f2d7-435b-9d27-ddbae23ad650",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM my_test_table\")\n",
    "display(df_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fd32a44-5b9d-4695-b7e2-2135064b6e1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###convert table into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e209c46-7dbb-416c-bd20-a0deacadc67b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import packages to run below code\n",
    "from pyspark.sql.functions import col\n",
    "sql2df=spark.table(\"my_test_table\")\n",
    "display(sql2df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8e3fd6c-b9f2-493c-b6e0-90a3fa816b8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###catalog\n",
    "Unity Catalog in Databricks\n",
    "\n",
    "Unity Catalog is a unified governance solution for all data and AI assets, including files, tables, and machine learning models in your Databricks workspace. It provides a centralized governance model that standardizes data governance across all data assets.\n",
    "\n",
    "Let's explore some basic operations with Unity Catalog.\n",
    "\n",
    "1. Create a Catalog\n",
    "A catalog is the highest level of organization in Unity Catalog. It contains schemas (databases), which in turn contain tables and views.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2a73b62-728c-4bc2-bb91-2fe92ae4404c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--Create a catalog using SQL\n",
    "create catalog if not exists my_catalog;\n",
    "\n",
    "\n",
    "--switch to the newly created catalog\n",
    "use catalog my_catalog;\n",
    "\n",
    "--2. Create a Schema (Database) within the Catalog.Schemas are used to organize tables and views within a catalog.\n",
    "\n",
    "create schema if not exists my_schema;\n",
    "\n",
    "--switch to the newly created schema\n",
    "use schema my_schema;\n",
    "\n",
    "--3. Create a Table within the Schema.Tables are used to store structured data.\n",
    "--Create a table using SQL\n",
    "drop table if exists my_test;\n",
    "create table if not exists my_test(\n",
    "  name string,\n",
    "  salary int,\n",
    "  gender string\n",
    ");\n",
    "    \n",
    "--4. Insert data into the table using SQL\n",
    "insert into my_test values\n",
    "(\"vikky\",10000,\"male\"),(\"sam\",20000,\"female\"),(\"hema\",30000,\"female\");\n",
    "select * from my_test;\n",
    "    \n",
    "--5. Create a view within the schema.Views are used to present a subset of data from a table or another view.\n",
    "--Create a view using SQL\n",
    "--create or replace view my_view as select * from my_test where gender = \"female\";\n",
    "    \n",
    "--6. Query the table using SQL\n",
    "select * from my_test;\n",
    "--select * from my_view;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b9cd31-7327-4e19-9071-16286a4b0afc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#7. Interact with the Table using Python\n",
    "#Load the table into a DataFrame\n",
    "\n",
    "df = spark.sql('select * from my_catalog.my_schema.my_test')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6b9d0a8-7d5e-4cf7-9f16-adf5c4592786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "pwd"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7926056474327527,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "databases and catalog operations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
