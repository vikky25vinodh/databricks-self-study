{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7db46d4e-897d-4f09-9116-8a11e06c2cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###some basic join concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3649d5af-d014-4d40-94cd-3655b76387a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Joins in Spark DataFrames\n",
    "df1=spark.createDataFrame([(1581,\"A\",\"Infosys\"),(1582,\"B\",\"google\"),(1583,\"C\",\"infratech\"),(1584,\"D\",\"jujutsu\")],[\"id\",\"name\",\"company\"])\n",
    "display(df1)\n",
    "display(df2)\n",
    "\n",
    "df2=spark.createDataFrame([(1581,\"India\"),(1582,\"USA\"),(1583,\"UK\")],[\"id\",\"country\"])\n",
    "\n",
    "#inner-join:\n",
    "inner_join = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "inner_join.show()\n",
    "\n",
    "#left-outer-join:\n",
    "left_outer_join = df1.join(df2, on=\"id\", how=\"left\")\n",
    "left_outer_join.show()\n",
    "\n",
    "#right_join:\n",
    "right_outer_join = df1.join(df2, on=\"id\", how=\"right\")\n",
    "right_outer_join.show()\n",
    "\n",
    "#full_outer_join:\n",
    "full_outer_join=df1.join(df2, on=\"id\", how=\"outer\")\n",
    "full_outer_join.show()\n",
    "\n",
    "#cross_join\n",
    "cross_join=df1.crossJoin(df2)\n",
    "cross_join.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd354d5-8b75-45e0-925f-038c9f21bd33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cross join returns the Cartesian product of two DataFrames, combining each row of the first DataFrame with every row of the second DataFrame.\n",
    "cross_join = df1.crossJoin(df2)\n",
    "display(cross_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d0d4cec-b830-44c7-b4ce-4175cd7ebc0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inner join returns only the rows that have matching values in both DataFrames.\n",
    "inner_join = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "display(inner_join)\n",
    "\n",
    "# Left outer join returns all rows from the left DataFrame, and the matched rows from the right DataFrame. \n",
    "# If no match is found, the result is NULL on the side of the right DataFrame.\n",
    "left_outer_join = df1.join(df2, on=\"id\", how=\"left\")\n",
    "display(left_outer_join)\n",
    "\n",
    "# Right outer join returns all rows from the right DataFrame, and the matched rows from the left DataFrame. \n",
    "# If no match is found, the result is NULL on the side of the left DataFrame.\n",
    "right_outer_join = df1.join(df2, on=\"id\", how=\"right\")\n",
    "display(right_outer_join)\n",
    "\n",
    "# Full outer join returns all rows when there is a match in either left or right DataFrame. \n",
    "# Rows without a match in one of the DataFrames will have NULLs in the respective columns.\n",
    "full_outer_join = df1.join(df2, on=\"id\", how=\"outer\")\n",
    "display(full_outer_join)\n",
    "\n",
    "# Cross join returns the Cartesian product of two DataFrames, combining each row of the first DataFrame with every row of the second DataFrame.\n",
    "cross_join = df1.crossJoin(df2)\n",
    "display(cross_join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e98ae083-0af2-436e-ab4d-86af2ac469f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###some basic Union concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fc6d06-60e9-4393-83b9-94575647ca50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfa=spark.createDataFrame([(\"vikky\",\"m\"),(\"sam\",\"f\"),(\"vinodh\",\"m\"),(\"hema\",\"f\")],[\"name\",\"gender\"])\n",
    "display(dfa)\n",
    "dfb=spark.createDataFrame([(\"vikky\",\"vpg\"),(\"sam\",\"ptr\"),(\"vinodh\",\"ptr\"),(\"hema\",\"dmc\"),(\"vikky\",\"vpg\")],[\"name\",\"village\"])\n",
    "display(dfb)\n",
    "\n",
    "\n",
    "\n",
    "# Union combines the rows of two DataFrames with the same schema.\n",
    "union_df = dfa.union(dfb)\n",
    "display(union_df)\n",
    "\n",
    "# Union all (same as union in Spark) includes duplicate rows.\n",
    "union_all_df = dfa.unionAll(dfb)\n",
    "display(union_all_df)\n",
    "\n",
    "# To remove duplicates after union, use distinct().\n",
    "union_distinct_df = dfa.union(dfb).distinct()\n",
    "display(union_distinct_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "joins and unions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
