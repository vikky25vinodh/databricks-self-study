{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "062e10e4-0342-4aa5-bd7b-a415d83242b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###● Describe the relationship between the data lakehouse and the data warehouse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53dbf96f-8b7f-4431-a08d-3c0d6f2d5da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# It provides the low-cost storage and flexibility of a data lake with the management and performance features of a data warehouse.\n",
    "# Data lakehouses enable organizations to store structured, semi-structured, and unstructured data in a single system, supporting analytics and BI workloads.\n",
    "# Data warehouses are optimized for structured data and analytics, while lakehouses support broader data types and use cases.\n",
    "# Lakehouses often use open formats (like Delta Lake) to provide ACID transactions, schema enforcement, and governance similar to data warehouses.\n",
    "\n",
    "description = \"\"\"\n",
    "The data lakehouse is an architecture that unifies the capabilities of data lakes and data warehouses. \n",
    "It allows organizations to store all types of data (structured, semi-structured, unstructured) in a single repository, \n",
    "while providing data management, reliability, and performance features traditionally found in data warehouses. \n",
    "This enables advanced analytics, machine learning, and business intelligence on a single platform.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Lakehouse vs Warehouse Relationship\": description})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecdf1642-e431-4fef-b687-eed273eeffa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify the improvement in data quality in the data lakehouse over the data lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f80e4d1-a01a-43f9-96ac-df8124db5b48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "improvement = \"\"\"\n",
    "Data lakehouses improve data quality over traditional data lakes by introducing ACID transactions, schema enforcement, and data governance features. \n",
    "These capabilities ensure data consistency, reliability, and integrity, reducing issues like data corruption, inconsistent schemas, and lack of lineage that are common in data lakes.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Data Quality Improvement in Lakehouse\": improvement})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca74b48f-bb32-40f8-9708-ce995c7603a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Compare and contrast silver and gold tables, which workloads will use a bronze table as a source, which workloads will use a gold table as a source.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e157f6f-3c47-4f5c-9395-dd9807a27241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "comparison = \"\"\"\n",
    "Silver tables are cleansed and enriched datasets, typically used for data exploration, reporting, and as a foundation for further transformations. Gold tables are highly curated, business-level aggregates or data marts, optimized for analytics, BI dashboards, and machine learning.\n",
    "\n",
    "Workloads using bronze tables as a source: ETL pipelines, data cleansing, and enrichment processes (moving data from raw to silver).\n",
    "Workloads using gold tables as a source: Business intelligence, reporting, advanced analytics, and machine learning models that require trusted, aggregated data.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Silver vs Gold Tables & Workload Sources\": comparison})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a0f3324-09c6-45d9-82e1-2ed11847b5df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify elements of the Databricks Platform Architecture, such as what is located in the data plane versus the control plane and what resides in the customer’s cloud account\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0037cfe2-f1eb-4b99-b57a-225d295c4b87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "architecture = {\n",
    "    \"Control Plane\": [\n",
    "        \"Databricks workspace UI\",\n",
    "        \"Job scheduling and orchestration\",\n",
    "        \"Cluster management services\",\n",
    "        \"Notebook management\",\n",
    "        \"Authentication and access control\"\n",
    "    ],\n",
    "    \"Data Plane\": [\n",
    "        \"Compute clusters (VMs/instances)\",\n",
    "        \"Data processing engines (Spark, DBSQL, ML runtimes)\",\n",
    "        \"Data storage access (to customer-managed storage)\",\n",
    "        \"Execution of user code and jobs\"\n",
    "    ],\n",
    "    \"Customer's Cloud Account\": [\n",
    "        \"Data plane resources (compute clusters, VMs)\",\n",
    "        \"Customer-managed data storage (e.g., S3, ADLS, GCS)\",\n",
    "        \"Networking and security configurations\",\n",
    "        \"Encryption keys and IAM roles\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "display({\"Databricks Platform Architecture Elements\": architecture})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b112af-1bb0-4031-9a92-03f2e7b807a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Differentiate between all-purpose clusters and jobs clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7c36d56-e1df-4669-abdb-e6cd5ecac186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "difference = \"\"\"\n",
    "All-purpose clusters are interactive, reusable clusters designed for collaborative data analysis, ad hoc queries, and development in notebooks. They remain active until manually terminated and can be shared by multiple users.\n",
    "\n",
    "Jobs clusters are ephemeral, single-use clusters created automatically for running production jobs or scheduled workflows. They are terminated automatically after the job completes, ensuring resource isolation and cost efficiency.\n",
    "\"\"\"\n",
    "\n",
    "display({\"All-Purpose Clusters vs Jobs Clusters\": difference})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac7b86e3-2406-453b-888e-508befcc55a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify how cluster software is versioned using the Databricks Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1423eb4a-b17c-47fb-877d-80c644a349f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "runtime_versioning_info = \"\"\"\n",
    "Databricks clusters use the Databricks Runtime, which is versioned using a major.minor format (e.g., 12.2, 13.3 LTS). \n",
    "\n",
    "- Major versions (e.g., 12.x to 13.x) indicate significant changes, possibly not backward-compatible.\n",
    "- Feature (minor) versions (e.g., 13.2 to 13.3) are backward-compatible enhancements within a major version.\n",
    "- LTS (Long Term Support) versions (e.g., 13.3 LTS) receive extended support.\n",
    "\n",
    "Each Databricks Runtime version bundles specific libraries, system components, and features. The runtime version is selected when creating a cluster.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Databricks Runtime Versioning\": runtime_versioning_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c7a6f96-8d0f-42f7-9351-7484b3809b89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify how clusters can be filtered to view those that are accessible by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecdafe2b-13bd-460d-ac08-57cad4993dfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cluster_filtering_info = \"\"\"\n",
    "Clusters accessible by a user can be filtered using the Databricks REST API or the Databricks UI.\n",
    "\n",
    "- In the Databricks UI, the Clusters page lists only those clusters the user has permission to view or attach to, based on cluster-level access control.\n",
    "- Using the REST API (`2.0/clusters/list`), the response includes only clusters the user is authorized to access. The API enforces permissions, so users see only clusters they can manage or attach to.\n",
    "\n",
    "To programmatically filter clusters accessible by the current user in a Databricks notebook, use the Databricks REST API with a personal access token. The API response will automatically be scoped to the user's permissions.\n",
    "\n",
    "Example (Python with Databricks Utilities):\n",
    "\"\"\"\n",
    "\n",
    "display({\"Filtering Accessible Clusters\": cluster_filtering_info})\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Replace with your Databricks workspace URL and personal access token\n",
    "workspace_url = \"https://<databricks-instance>\"\n",
    "token = dbutils.secrets.get(scope=\"my_scope\", key=\"databricks_token\")\n",
    "\n",
    "response = requests.get(\n",
    "    f\"{workspace_url}/api/2.0/clusters/list\",\n",
    "    headers={\"Authorization\": f\"Bearer {token}\"}\n",
    ")\n",
    "\n",
    "clusters = response.json().get(\"clusters\", [])\n",
    "display([{\"cluster_name\": c[\"cluster_name\"], \"cluster_id\": c[\"cluster_id\"]} for c in clusters])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09928bdd-c06c-4ef8-89be-365c9d77b927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Describe how clusters are terminated and the impact of terminating a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "def4e8a9-5f28-48cd-bea0-b384210511fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cluster_termination_info = \"\"\"\n",
    "Clusters in Databricks can be terminated manually via the UI, programmatically using the REST API, or automatically based on configured auto-termination settings.\n",
    "\n",
    "- Manual Termination: Users with appropriate permissions can terminate clusters from the Clusters UI or by calling the `2.0/clusters/delete` REST API endpoint.\n",
    "- Auto-Termination: Clusters can be configured to automatically terminate after a specified period of inactivity.\n",
    "\n",
    "Impact of Terminating a Cluster:\n",
    "- All active jobs, notebooks, and interactive sessions running on the cluster are immediately stopped.\n",
    "- Unsaved results or in-memory data are lost.\n",
    "- The cluster's compute resources are released, and billing for those resources stops.\n",
    "- Metadata, libraries, and configuration are preserved for future restarts, but ephemeral data is lost.\n",
    "\n",
    "Terminating a cluster is a non-reversible operation for the running state; workloads must be restarted on a new or restarted cluster.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Cluster Termination and Impact\": cluster_termination_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c66fa4c3-5e7c-434b-a1fa-05c9e1dd684f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Identify a scenario in which restarting the cluster will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe67834f-440e-4626-b18c-7f8b632ad656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "restart_scenario_info = \"\"\"\n",
    "Restarting a Databricks cluster is useful when you need to clear the cluster's state, such as after updating installed libraries, environment variables, or configuration settings. For example, if you upgrade a Python package or change an environment variable, a cluster restart ensures that all nodes use the new configuration, resolving issues caused by stale dependencies or inconsistent environments.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Scenario Where Restarting Cluster is Useful\": restart_scenario_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41b2e06e-99f4-4c69-a009-c5fb6f657ac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Describe how to use multiple languages within the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b11e1f2-266c-457b-8ec6-ebab9fa8ce9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "multi_language_info = \"\"\"\n",
    "Databricks notebooks support multiple languages within the same notebook by using language magic commands at the beginning of a cell. The supported languages include Python (`%python`), SQL (`%sql`), Scala (`%scala`), and R (`%r`). By default, cells use the notebook's primary language, but you can switch languages per cell using these magic commands.\n",
    "\n",
    "For example:\n",
    "- `%python` for Python code\n",
    "- `%sql` for SQL queries\n",
    "- `%scala` for Scala code\n",
    "- `%r` for R code\n",
    "\n",
    "Variables defined in one language can sometimes be accessed in another using Databricks utilities like `dbutils.notebook.run` or by explicitly saving and loading data between cells.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Using Multiple Languages in a Notebook\": multi_language_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c40bb957-69d5-47d8-9d15-83cf743b4b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify how to run one notebook from within another notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fcabc78-bd28-4355-8506-6842940f8b0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "run_notebook_info = \"\"\"\n",
    "You can run one Databricks notebook from within another notebook using the `dbutils.notebook.run` command. This allows you to modularize your code and reuse notebooks as functions or scripts.\n",
    "\n",
    "Example usage:\n",
    "dbutils.notebook.run(\"notebook_path\", timeout_seconds, arguments)\n",
    "\n",
    "- \"notebook_path\": The path to the notebook you want to run.\n",
    "- timeout_seconds: Maximum time in seconds to wait for the notebook to finish.\n",
    "- arguments: (Optional) A dictionary of arguments to pass to the notebook.\n",
    "\n",
    "The called notebook can access the arguments using `dbutils.widgets.get(\"argument_name\")`.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Running One Notebook from Another\": run_notebook_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85d2d7ec-5e5c-46eb-8521-49974a07f6cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify how notebooks can be shared with others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c626249b-be07-4265-a438-f4d4fdee995f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebook_sharing_info = \"\"\"\n",
    "Databricks notebooks can be shared with others in several ways:\n",
    "\n",
    "1. **Workspace Permissions**: Share notebooks by adjusting permissions in the Databricks workspace. You can grant view, edit, or run access to users or groups.\n",
    "\n",
    "2. **Export and Import**: Export notebooks as `.dbc`, `.ipynb`, or `.py` files and share them externally. Recipients can import these files into their own Databricks workspace.\n",
    "\n",
    "3. **URL Sharing**: Share the notebook's URL with users who have appropriate workspace access.\n",
    "\n",
    "4. **Version Control Integration**: Use Git integration to share and collaborate on notebooks through repositories.\n",
    "\n",
    "Choose the method that best fits your collaboration and security requirements.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Sharing Databricks Notebooks\": notebook_sharing_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "afcf29dc-91dc-4503-914e-cafac112b4a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Describe how Databricks Repos enables CI/CD workflows in Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0250f7ec-f249-40f1-83b9-51e11fd130ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "repos_cicd_info = \"\"\"\n",
    "Databricks Repos integrates Git repositories directly into the Databricks workspace, enabling robust CI/CD workflows:\n",
    "\n",
    "1. **Source Control Integration**: Connect notebooks, libraries, and other files to Git providers (GitHub, GitLab, Azure DevOps, Bitbucket), ensuring version control and collaboration.\n",
    "\n",
    "2. **Branching and Pull Requests**: Use Git branches for feature development, bug fixes, and code reviews. Pull requests facilitate code review and automated testing before merging.\n",
    "\n",
    "3. **Automated Testing**: Integrate with CI tools (e.g., GitHub Actions, Azure Pipelines) to run automated tests on code changes, ensuring code quality and reliability.\n",
    "\n",
    "4. **Continuous Deployment**: Use CD pipelines to deploy notebooks, jobs, and other artifacts to Databricks environments automatically after successful tests and reviews.\n",
    "\n",
    "5. **Traceability and Auditability**: Track changes, authorship, and history of all code and configuration changes, supporting compliance and reproducibility.\n",
    "\n",
    "Databricks Repos streamlines the development lifecycle, enabling collaborative, automated, and reliable delivery of analytics and machine learning solutions.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Databricks Repos and CI/CD Workflows\": repos_cicd_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c2533ad-345a-4855-b13d-596454f37ac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify Git operations available via Databricks Repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b0980be-e890-4d68-832a-e23d22e3c73b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "git_operations_info = \"\"\"\n",
    "Databricks Repos supports the following Git operations:\n",
    "\n",
    "1. **Clone**: Clone remote Git repositories into the Databricks workspace.\n",
    "2. **Pull**: Fetch and merge changes from the remote repository to keep local content up to date.\n",
    "3. **Commit**: Record changes to tracked files in the local repository.\n",
    "4. **Push**: Send committed changes from the local repository to the remote repository.\n",
    "5. **Branch**: Create, switch, and manage branches for feature development or bug fixes.\n",
    "6. **Merge**: Merge changes from one branch into another.\n",
    "7. **Revert**: Undo changes by reverting commits.\n",
    "8. **Resolve Conflicts**: Address merge conflicts directly within the Databricks UI.\n",
    "9. **View History**: Inspect commit history and diffs for files and notebooks.\n",
    "10. **Pull Requests (via Git provider)**: Initiate and manage pull requests through the connected Git provider.\n",
    "\n",
    "These operations enable collaborative development and version control directly within Databricks.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Git Operations in Databricks Repos\": git_operations_info})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ab9c381-81d6-4e90-af0a-1c90b7716351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify limitations in Databricks Notebooks version control functionality relative to Repos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7959bb88-a5bb-4993-8318-d3b229402784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "notebooks_vc_limitations = \"\"\"\n",
    "Limitations of Databricks Notebooks version control relative to Repos:\n",
    "\n",
    "1. **Granularity**: Native notebook version control only tracks changes at the notebook level, lacking file-level and multi-file project support.\n",
    "2. **Collaboration**: No support for branching, merging, or pull requests, limiting collaborative workflows.\n",
    "3. **Integration**: Cannot integrate with external Git providers (e.g., GitHub, GitLab, Azure DevOps) for CI/CD, automated testing, or deployment.\n",
    "4. **History and Traceability**: Limited commit history and diff capabilities compared to Git-based workflows.\n",
    "5. **Project Structure**: No support for organizing code across multiple files or projects as in a Git repo.\n",
    "6. **Automation**: Lacks hooks for automated workflows (e.g., CI/CD pipelines) triggered by code changes.\n",
    "7. **Conflict Resolution**: No built-in tools for resolving merge conflicts.\n",
    "8. **Auditability**: Limited audit trails and change tracking compared to Git-backed Repos.\n",
    "\n",
    "Databricks Repos addresses these limitations by providing full Git integration, enabling robust version control, collaboration, and automation.\n",
    "\"\"\"\n",
    "\n",
    "display({\"Databricks Notebooks Version Control Limitations vs Repos\": notebooks_vc_limitations})"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Section 1: Databricks Lakehouse Platform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}