{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f108a42b-9ced-4da9-bd61-dc3658921e59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Extract data from a single file and from a directory of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f95d85-c636-45af-b7e1-0d9b3e48370d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_single = spark.read.parquet(\"/path/to/single/file.parquet\")\n",
    "display(df_single)\n",
    "\n",
    "df_dir = spark.read.parquet(\"/path/to/directory/\")\n",
    "display(df_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "152af7b4-6854-4a81-b8da-4b2bdfc3a758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify the prefix included after the FROM keyword as the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8023b6cb-db32-4dbd-9912-8503d755743f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Common prefixes/ways to specify the data source after FROM in Databricks SQL\n",
    "\n",
    "-- 1. Table Names (qualified or unqualified)\n",
    "SELECT * FROM my_table;\n",
    "SELECT * FROM my_schema.my_table;\n",
    "SELECT * FROM my_catalog.my_schema.my_table;\n",
    "\n",
    "-- 2. File Paths (external files)\n",
    "SELECT * FROM 'dbfs:/path/to/my/file.csv';\n",
    "SELECT * FROM 's3://bucket/path/to/data/';\n",
    "SELECT * FROM 'abfss://container@storageaccount.dfs.core.windows.net/path/to/data/';\n",
    "\n",
    "-- 3. Views (temporary or permanent)\n",
    "SELECT * FROM my_view;\n",
    "SELECT * FROM my_temp_view;\n",
    "\n",
    "-- 4. Subqueries\n",
    "SELECT * FROM (SELECT col1, col2 FROM another_table) AS subquery_alias;\n",
    "\n",
    "-- 5. Table-valued functions (TVFs)\n",
    "SELECT * FROM read_files('path/to/files/');\n",
    "\n",
    "-- 6. VALUES clause (in-memory table)\n",
    "SELECT * FROM VALUES (1, 'A'), (2, 'B') AS my_data(id, value);\n",
    "\n",
    "-- 7. AS OF syntax (Delta Lake Time Travel)\n",
    "SELECT * FROM my_delta_table TIMESTAMP AS OF '2023-01-01';\n",
    "SELECT * FROM my_delta_table VERSION AS OF 123;\n",
    "SELECT * FROM my_delta_table@20230101000000000;\n",
    "SELECT * FROM my_delta_table@v123;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fea06a4b-6ca4-4348-879b-acacb0bcde35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Create a view, a temporary view, and a CTE as a reference to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e577053-0dee-485f-a025-e66c0770195e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a permanent view from the parquet file\n",
    "df_single.write.mode(\"overwrite\").saveAsTable(\"permanent_view_single\")\n",
    "\n",
    "# Create a temporary view from the parquet file\n",
    "df_single.createOrReplaceTempView(\"temp_view_single\")\n",
    "\n",
    "# Use a CTE to reference the file in a SQL query\n",
    "query = \"\"\"\n",
    "WITH cte_single AS (\n",
    "  SELECT * FROM parquet.`/path/to/single/file.parquet`\n",
    ")\n",
    "SELECT * FROM cte_single\n",
    "\"\"\"\n",
    "df_cte = spark.sql(query)\n",
    "display(df_cte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af38134b-6140-4af7-a0e6-10f3cb904a15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a permanent view from the parquet file\n",
    "CREATE OR REPLACE TABLE permanent_view_single\n",
    "USING PARQUET\n",
    "OPTIONS (path \"/path/to/single/file.parquet\")\n",
    "AS SELECT * FROM parquet.`/path/to/single/file.parquet`;\n",
    "\n",
    "-- Create a temporary view from the parquet file\n",
    "CREATE OR REPLACE TEMP VIEW temp_view_single AS\n",
    "SELECT * FROM parquet.`/path/to/single/file.parquet`;\n",
    "    \n",
    "-- Use a CTE to reference the file in a SQL query\n",
    "WITH cte_single AS (\n",
    "  SELECT * FROM parquet.`/path/to/single/file.parquet`\n",
    ")\n",
    "SELECT * FROM cte_single;\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27c970f3-7cc1-4639-ac6e-e8dcf3e343d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Identify that tables from external sources are not Delta Lake tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a99851a3-6030-4531-9ef0-2189a7e28629",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if the tables are Delta Lake tables by inspecting their table properties\n",
    "\n",
    "# Permanent table\n",
    "permanent_table = \"permanent_view_single\"\n",
    "permanent_table_details = spark.sql(f\"DESCRIBE DETAIL {permanent_table}\")\n",
    "display(permanent_table_details)\n",
    "\n",
    "# Temporary view (not a table, so not Delta)\n",
    "# CTE result is not a table, so not Delta\n",
    "\n",
    "# For the parquet file directly\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def is_delta_table(path):\n",
    "    try:\n",
    "        return spark.read.format(\"delta\").load(path)._jdf.isDelta()\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "is_delta_single = is_delta_table(\"/path/to/single/file.parquet\")\n",
    "is_delta_dir = is_delta_table(\"/path/to/directory/\")\n",
    "\n",
    "print(f\"Is '/path/to/single/file.parquet' a Delta table? {is_delta_single}\")\n",
    "print(f\"Is '/path/to/directory/' a Delta table? {is_delta_dir}\")\n",
    "\n",
    "# Explanation:\n",
    "# - Tables created from external sources like Parquet files are not Delta Lake tables unless explicitly written in Delta format.\n",
    "# - Delta tables have transaction logs (_delta_log) and support ACID transactions, schema enforcement, and time travel.\n",
    "# - Parquet tables lack these features and are considered external, non-Delta tables.\n",
    "# - DESCRIBE DETAIL on a Delta table shows 'Provider' as 'delta'; for non-Delta, it is 'parquet' or empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53c2adf9-78f7-4d4a-ae93-049af4ba567a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- Check if the tables are Delta Lake tables by inspecting their table properties\n",
    "\n",
    "-- Permanent table\n",
    "DESCRIBE DETAIL permanent_view_single;\n",
    "\n",
    "-- Temporary view (not a table, so not Delta)\n",
    "-- CTE result is not a table, so not Delta\n",
    "\n",
    "-- For the parquet file directly\n",
    "DESCRIBE DETAIL parquet.`/path/to/single/file.parquet`;\n",
    "DESCRIBE DETAIL parquet.`/path/to/directory/`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45640503-01db-4ad7-8dca-a2122e3fb767",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Create a table from a JDBC connection and from an external CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e490f09-f2f7-4266-b8d4-12f76fc80238",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a table from a JDBC connection\n",
    "CREATE OR REPLACE TABLE jdbc_table\n",
    "USING JDBC\n",
    "OPTIONS (\n",
    "  url 'jdbc:postgresql://hostname:5432/database',\n",
    "  dbtable 'schema.source_table',\n",
    "  user 'your_username',\n",
    "  password 'your_password'\n",
    ");\n",
    "\n",
    "-- Create a table from an external CSV file\n",
    "CREATE OR REPLACE TABLE csv_table\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path 'dbfs:/path/to/external/file.csv',\n",
    "  header 'true',\n",
    "  inferSchema 'true'\n",
    ")\n",
    "AS SELECT * FROM csv.`dbfs:/path/to/external/file.csv`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "462b0794-4104-458a-a5b5-c0b873abda81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify how the count_if function and the count where x is null can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "035ce14a-b5d6-4110-8138-ef2f444ad3a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Using count_if to count rows where column x is null\n",
    "SELECT count_if(x IS NULL) AS null_count\n",
    "FROM permanent_view_single;\n",
    "\n",
    "-- Using count to count rows where column x is null\n",
    "SELECT count(*) AS null_count\n",
    "FROM permanent_view_single\n",
    "WHERE x IS NULL;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8216d268-0764-4745-935d-13a55a068f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Identify how the count(row) skips NULL values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07d2a181-6b40-4733-be4a-e6e15c2b72a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Demonstrate how COUNT(column) skips NULL values\n",
    "\n",
    "SELECT\n",
    "  COUNT(x) AS count_non_null_x,      -- counts only non-NULL values in column x\n",
    "  COUNT(*) AS count_all_rows,        -- counts all rows, including those where x is NULL\n",
    "  COUNT(CASE WHEN x IS NULL THEN 1 END) AS count_null_x  -- counts only rows where x is NULL\n",
    "FROM permanent_view_single;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed9ad77f-2d41-41fe-b43b-4cdef1b44676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Deduplicate rows from an existing Delta Lake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99b8c601-f0e7-4eb6-bcee-f8d195a002c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Deduplicate rows in a Delta Lake table based on all columns\n",
    "\n",
    "CREATE OR REPLACE TABLE deduplicated_table AS\n",
    "SELECT DISTINCT *\n",
    "FROM existing_delta_table;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Section 2: ELT with Apache Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}