{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfaad047-e724-45f4-bb49-e68e9406a6c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Describe a DataFrame\n",
    "\n",
    "Your data processing in Apaceh Spark is accomplished by defining Dataframes to read and process the Data.\n",
    "\n",
    "This notebook will introduce how to read your data using Apache Spark Dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "582e0d4d-c49c-463f-8546-5655b8fd0295",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Introduction\n",
    "\n",
    "**Data Source**\n",
    "* One hour of Pagecounts from the English Wikimedia projects captured August 5, 2016, at 12:00 PM UTC.\n",
    "* Size on Disk: ~23 MB\n",
    "* Type: Compressed Parquet File\n",
    "* More Info: <a href=\"https://dumps.wikimedia.org/other/pagecounts-raw\" target=\"_blank\">Page view statistics for Wikimedia projects</a>\n",
    "\n",
    "**Technical Accomplishments:**\n",
    "* Develop familiarity with the `DataFrame` APIs\n",
    "* Introduce the classes...\n",
    "  * `SparkSession`\n",
    "  * `DataFrame` (aka `Dataset[Row]`)\n",
    "* Introduce the actions...\n",
    "  * `count()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35dc62e4-3218-41ed-a85e-d961141d9a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4af4f7-24ed-4e90-85de-462c09b45752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb5700d2-690b-43bd-afce-66a14075bf33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"Describe Dataframe\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16dd199a-ddad-42a7-a84c-4779ab811222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.get('spark.app.name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0282e1-8393-4a54-8f11-1c807b2ad153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "539cbf7b-54dd-4dcb-b350-ab579ae73b81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **The Data Source**\n",
    "\n",
    "* In this notebook, we will be using a compressed parquet \"file\" called **pagecounts** (~23 MB file from Wikipedia)\n",
    "* We will explore the data and develop an understanding of it as we progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b0e9a9-b796-4a93-a72e-ef73cde8897b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../DatasetSourcePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f62f58d-07e2-4eb1-8ed3-f78a561d2f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquetDir = sourcePath + \"/dataset/pagecounts/staging_parquet_en_only_clean/\"\n",
    "\n",
    "file_list = dbutils.fs.ls(parquetDir)\n",
    "for f in file_list:\n",
    "  print(f.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ddb6668-33ce-42f7-9902-d8a42e62fe20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see from the files listed above, this data is stored in <a href=\"https://parquet.apache.org\" target=\"_blank\">Parquet</a> files which can be read in a single command, the result of which will be a `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74020868-ad23-4ad5-bf43-3036c4ac8ab2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create a DataFrame\n",
    "* We can read the Parquet files into a `DataFrame`.\n",
    "* We'll start with the object **spark**, an instance of `SparkSession` and the entry point to Spark 2.0 applications.\n",
    "* From there we can access the `read` object which gives us an instance of `DataFrameReader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7caea083-8cac-4b7e-9e8b-a85f3c4e5bef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquetDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb4fe6a-d412-4400-9603-f69a7ad089b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF = (spark  # SparkSession & Entry Point\n",
    "  .read                     # DataFrameReader\n",
    "  .parquet(parquetDir)      # Returns an instance of DataFrame\n",
    ")\n",
    "\n",
    "print(pagecountsEnAllDF)    # Python hack to see the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34f65436-24dd-407c-9a7a-a9286bbddb44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF = spark \\\n",
    "  .read \\\n",
    "  .parquet(parquetDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e827cad6-b644-488e-8846-4ba2fa0eb2bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## count()\n",
    "\n",
    "If you look at the API docs, `count()` is described like this:\n",
    "> Returns the number of rows in the Dataset.\n",
    "\n",
    "`count()` will trigger a job to process the request and return a value.\n",
    "\n",
    "We can now count all records in our `DataFrame` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de0a0822-ce3d-4bfe-a01d-2f8646a5a5a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total = pagecountsEnAllDF.count()\n",
    "\n",
    "print(\"Record Count: {0:,}\".format( total ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f53f346-b98b-4c1a-a949-1da639770d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "That tells us that there are around 2 million rows in the `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f65af400-38e5-4553-9d7d-e7e061edd51c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Use common DataFrame methods\n",
    "\n",
    "We will now build upon that concept by introducing common DataFrame methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bc509c-8ea9-4088-a209-0d6a783d6f64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Our Data\n",
    "\n",
    "Let's continue by taking a look at the type of data we have. \n",
    "\n",
    "We can do this with the `printSchema()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9de1e7-f949-4db7-9cbb-532316d46bd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cb17494-5dff-456d-9461-886c209805b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We should now be able to see that we have four columns of data:\n",
    "* **project** (*string*): The name of the Wikipedia project. This will include values such as:\n",
    "  * **en**: The English version of Wikipedia.\n",
    "  * **fr**: The French version of Wikipedia.\n",
    "  * **en.d**: The English version of Wiktionary.\n",
    "  * **fr.b**: The French version of Wikibooks.\n",
    "  * **de.n**: The German version of Wikinews.\n",
    "* **article** (*string*): The name of the article in the corresponding project. This will include values such as:\n",
    "  * Apache_Spark\n",
    "  * Matei_Zaharia\n",
    "  * Kevin_Bacon\n",
    "* **requests** (*integer*): The number of requests (clicks) the article has received in the hour this data represents.\n",
    "* **bytes_served** (*long*): The total number of bytes delivered for the requested article.\n",
    "  > **Note:** In our copy of the data, this value is zero for all records and consequently is of no value to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8d56ed-8e65-4892-9268-9fd71ed9460d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark API\n",
    "\n",
    "You have already seen one command available to the `DataFrame` class, namely `DataFrame.printSchema()`\n",
    "  \n",
    "Let's take a look at the API to see what other operations we have available.\n",
    "\n",
    "* <a href=\"https://spark.apache.org/docs/latest/\" target=\"_blank\">Spark API Documentation - Latest</a>\n",
    "\n",
    "### Spark API (Python)\n",
    "\n",
    "0. Select **Spark Python API (Sphinx)**.\n",
    "0. Look for `Spark SQL API Reference`\n",
    "0. Look up the documentation for `pyspark.sql.DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c15ebdd-8621-45cf-8dca-1d5f79ad0f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Use the Display function\n",
    "\n",
    "There are different ways to view data in a DataFrame. This notebook covers these methods as well as transformations to further refine the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fe089b9-8279-48e6-a4c9-8ddf2722c78f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Technical Accomplishments:**\n",
    "* Introduce the transformations...\n",
    "  * `limit(..)`\n",
    "  * `select(..)`\n",
    "  * `drop(..)`\n",
    "  * `distinct()`\n",
    "  * `dropDuplicates(..)`\n",
    "* Introduce the actions...\n",
    "  * `show(..)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04645024-deff-403a-9934-eea2e07891e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## show(..)\n",
    "\n",
    "What we want to look for next is a function that will allow us to print the data to the console.\n",
    "\n",
    "In the API docs for `DataFrame`/`Dataset` find the docs for the `show(..)` command(s).\n",
    "\n",
    "In the case of Python, we have one method with two optional parameters.<br/>\n",
    "In the case of Scala, we have several overloaded methods.<br/>\n",
    "\n",
    "In either case, the `show(..)` method effectively has two optional parameters:\n",
    "* **n**: The number of records to print to the console, the default being 20.\n",
    "* **truncate**: If true, columns wider than 20 characters will be truncated, where the default is true.\n",
    "\n",
    "Let's take a look at the data in our `DataFrame` with the `show()` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "835f0a6a-4eb0-4f0e-8d4c-d30d95903345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b31d75-dccb-4278-b992-aa5c9314eabc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In the cell above, change the parameters of the show command to:\n",
    "* print only the first five records\n",
    "* disable truncation\n",
    "* print only the first ten records and disable truncation\n",
    "\n",
    "**Note:** The function `show(..)` is an **action** which triggers a job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd1d0d07-1944-402f-b203-d0af34e62df3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF.show(20,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80db0516-c16e-4463-b11f-d3e1961fff5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f69fb51-25d2-4f55-aa3b-990fbe605559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## limit(..)\n",
    "\n",
    "Both `show(..)` and `display(..)` are **actions** that trigger jobs (though in slightly different ways).\n",
    "\n",
    "If you recall, `show(..)` has a parameter to control how many records are printed but, `display(..)` does not.\n",
    "\n",
    "We can address that difference with our first transformation, `limit(..)`.\n",
    "\n",
    "If you look at the API docs, `limit(..)` is described like this:\n",
    "> Returns a new Dataset by taking the first n rows...\n",
    "\n",
    "`show(..)`, like many actions, does not return anything. \n",
    "\n",
    "On the other hand, transformations like `limit(..)` return a **new** `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d2b470d-8284-4572-b7d8-cc088d188642",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "limitedDF = pagecountsEnAllDF.limit(5) # \"limit\" the number of records to the first 5\n",
    "\n",
    "limitedDF # Python hack to force printing of the data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec28e66a-1533-4675-bd58-a154dbd258ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Nothing Happened\n",
    "* Notice how \"nothing\" happened - that is no job was triggered.\n",
    "* This is because we are simply defining the second step in our transformations.\n",
    "  1. Read in the parquet file (represented by **pagecountsEnAllDF**).\n",
    "  1. Limit those records to just the first 5 (represented by **limitedDF**).\n",
    "* It's not until we induce an action that a job is triggered and the data is processed\n",
    "\n",
    "We can induce a job by calling `show(..)` actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e0c1ce6-7b23-4838-96e9-0f012df52a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "limitedDF.show(100, False) #show up to 100 records and don't truncate the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4de7f4-8525-42c5-ba9b-74eb6aa7102e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> you can enable `spark.sql.repl.eagerEval.enabled` configuration for the eager evaluation of PySpark DataFrame in notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a2dabdc-aad0-4789-aa22-830bd47bf8a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3257f202-bccb-402e-b078-fac93e7341ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "limitedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58ffd49e-a6ae-48b9-afe3-bde3cd15e62a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(pagecountsEnAllDF.limit(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905db408-13df-4292-81d4-c64a84d4d734",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## select(..)\n",
    "\n",
    "Let's say, for the sake of argument, that we don't want to look at all the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e6dc90-9ebf-4dcd-a6cc-887eaca52951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89180196-759e-492c-9196-64b43c5ab778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c72d7d4-fbe0-45f8-b0fa-015071941d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For example, it was asserted above that **bytes_served** had nothing but zeros in it and consequently is of no value to us.\n",
    "\n",
    "If that is the case, we can disregard it by selecting only the three columns that we want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae26faa-6606-43ff-8bbe-09af42e384bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform the data by selecting only three columns\n",
    "onlyThreeDF = (pagecountsEnAllDF\n",
    "  .select(\"project\", \"article\", \"requests\")\n",
    ")\n",
    "\n",
    "# Now let's take a look at what the schema looks like\n",
    "onlyThreeDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47969d68-244f-4841-b57b-76e18164148b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Again, notice how the call to `select(..)` does not trigger a job.\n",
    "\n",
    "That's because `select(..)` is a transformation. It's just one more step in a long list of transformations.\n",
    "\n",
    "Let's go ahead and invoke the action `show(..)` and take a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24666607-55aa-4f38-98ee-4f1691bcd599",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# And lastly, show the first five records which should exclude the bytes_served column.\n",
    "onlyThreeDF.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f794207-e0bf-4603-82f5-5f29b0f24a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The `select(..)` command is one of the most powerful and most commonly used transformations. \n",
    "\n",
    "We will see plenty of other examples of its usage as we progress.\n",
    "\n",
    "If you look at the API docs, `select(..)` is described like this:\n",
    "> Returns a new Dataset by computing the given Column expression for each element.\n",
    "\n",
    "The \"Column expression\" referred to there is where the true power of this operation shows up. Again, we will go deeper on these later.\n",
    "\n",
    "Just like `limit(..)`, `select(..)` \n",
    "* does not trigger a job\n",
    "* returns a new `DataFrame`\n",
    "* simply defines the next transformation in a sequence of transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c61221-6575-4c70-9458-6c3e09497273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## drop(..)\n",
    "\n",
    "As a quick side note, you will quickly discover there are a lot of ways to accomplish the same task.\n",
    "\n",
    "Take the transformation `drop(..)` for example - instead of selecting everything we wanted, `drop(..)` allows us to specify the columns we don't want.\n",
    "\n",
    "If you look at the API docs, `drop(..)` is described like this:\n",
    "> Returns a new Dataset with a column dropped.\n",
    "\n",
    "And we can see that we can produce the same result as the last exercise this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c97c348-c18f-4ab7-810d-2554bc5d18c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transform the data by selecting only three columns\n",
    "droppedDF = (pagecountsEnAllDF\n",
    "  .drop(\"bytes_served\")\n",
    ")\n",
    "# Now let's take a look at what the schema looks like\n",
    "droppedDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4cc609-63bb-4716-8603-9541e7c039d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Again, `drop(..)` is just one more transformation - that is no job is triggered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fab9199-1b26-43ee-b8d7-a8df844d06da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# And lastly, show the first five records which should exclude the bytes_served column.\n",
    "droppedDF.show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ee813d-cfa0-49f7-b652-d746cabce204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## distinct() & dropDuplicates()\n",
    "\n",
    "These two transformations do the same thing. In fact, they are aliases for one another.\n",
    "* You can see this by looking at the source code for these two methods\n",
    "* ```def distinct(): Dataset[T] = dropDuplicates()```\n",
    "\n",
    "The difference between them has everything to do with the programmer and their perspective.\n",
    "* The name **distinct** will resonate with developers, analyst and DB admins with a background in SQL.\n",
    "* The name **dropDuplicates** will resonate with developers that have a background or experience in functional programming.\n",
    "\n",
    "As you become more familiar with the various APIs, you will see this pattern reassert itself.\n",
    "\n",
    "The designers of the API are trying to make the API as approachable as possible for multiple target audiences.\n",
    "\n",
    "If you look at the API docs, both `distinct(..)` and `dropDuplicates(..)` are described like this:\n",
    "> Returns a new Dataset that contains only the unique rows from this Dataset....\n",
    "\n",
    "With this transformation, we can now tackle our first business question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e293b8b5-b8ad-453c-9823-de23f83db104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### How many different English Wikimedia projects saw traffic during that hour?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d89d528b-7d0c-4a71-860d-c17c3e1f58f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "If you recall, our original `DataFrame` has this schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0488b63-2c42-4064-b9bd-7c16b2a54e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6f0a19f-cb7c-4b96-a98d-e41f0cb22656",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The transformation `distinct()` is applied to the row as a whole - data in the **project**, **article** and **requests** column will effect this evaluation.\n",
    "\n",
    "To get the distinct list of projects, and only projects, we need to reduce the number of columns to just the one column, **project**. \n",
    "\n",
    "We can do this with the `select(..)` transformation and then we can introduce the `distinct()` transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79fa0783-4b4b-4858-b9d9-121a41b03356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "distinctDF = (pagecountsEnAllDF\n",
    "  .select(\"project\")\n",
    "  .distinct()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec4bf003-03a9-4d90-8eab-e6f56fa00838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Just to reinforce, we have three transformations:\n",
    "1. Read the data (now represented by `pagecountsEnAllDF`)\n",
    "1. Select just the one column\n",
    "1. Reduce the records to a distinct set\n",
    "\n",
    "No job is triggered until we perform an action like `show(..)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cf8fdf4-e623-4d7d-8b7a-e997296e3be0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# There will not be more than 100 projects\n",
    "distinctDF.show(100, False)               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "852c05a6-b151-4d2d-be97-32790795c008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can count those if you like.\n",
    "\n",
    "But, it would be easier to ask the `DataFrame` for the `count()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86021bd3-c950-4573-96f0-d64f18ddf7dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total = distinctDF.count()     \n",
    "print(\"Distinct Projects: {0:,}\".format( total ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdbf57a6-f24f-4785-90a6-3612ab804ddc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## dropDuplicates(columns...)\n",
    "\n",
    "The method `dropDuplicates(..)` has a second variant that accepts one or more columns.\n",
    "* The distinction is not performed across the entire record unlike `distinct()` or even `dropDuplicates()`.\n",
    "* The distinction is based only on the specified columns.\n",
    "* This allows us to keep all the original columns in our `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff34c00b-fc7a-40d0-8a2c-5ee5a4c7cb52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample customer DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", \"Smith\", \"NY\", 34),\n",
    "    (2, \"Bob\", \"Johnson\", \"CA\", 45),\n",
    "    (1, \"Alice\", \"Smith\", \"NY\", 34),\n",
    "    (3, \"David\", \"Lee\", \"TX\", 29)\n",
    "]\n",
    "\n",
    "columns = [\"CustomerID\", \"FirstName\", \"LastName\", \"State\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Using dropDuplicates method\n",
    "distinctDF = df.distinct() # or distinctDF = df.dropDuplicate() \n",
    "\n",
    "# Display the distinct DataFrame\n",
    "display(distinctDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc48ac8d-2900-48fc-93b2-dee9045209ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sample customer DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", \"Smith\", \"NY\", 34),\n",
    "    (2, \"Bob\", \"Johnson\", \"CA\", 45),\n",
    "    (1, \"Alice\", \"Smith\", \"New York\", 34),\n",
    "    (3, \"David\", \"Lee\", \"TX\", 29)\n",
    "]\n",
    "\n",
    "columns = [\"CustomerID\", \"FirstName\", \"LastName\", \"State\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Using dropDuplicates method\n",
    "distinctDF = df.dropDuplicates([\"CustomerID\", \"FirstName\", \"LastName\", \"Age\"])\n",
    "\n",
    "# Display the distinct DataFrame\n",
    "display(distinctDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b94b323-6c16-4f3c-a226-b72f4cca5d2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "Our code is spread out over many cells which can make this a little hard to follow.\n",
    "\n",
    "Let's take a look at the same code in a single cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f0f8cc0-77f5-434e-8842-a43d6aff8920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF = (spark       # SparkSession & Entry Point\n",
    "  .read                          # DataFrameReader\n",
    "  .parquet(parquetDir)           # Returns an instance of DataFrame\n",
    ")\n",
    "distinctDF = (pagecountsEnAllDF  # Our original DataFrame from spark.read.parquet(..)\n",
    "  .select(\"project\")             # Drop all columns except the \"project\" column\n",
    "  .distinct()                    # Reduce the set of all records to just the distinct column.\n",
    ")\n",
    "total = distinctDF.count()     \n",
    "print(\"Distinct Projects: {0:,}\".format( total ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aac8829-4239-4902-8f2b-54ebb3793d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## DataFrames vs SQL & Temporary Views\n",
    "\n",
    "The `DataFrame`s API is built upon an SQL engine.\n",
    "\n",
    "As such we can \"convert\" a `DataFrame` into a temporary view (or table) and then use it in \"standard\" SQL.\n",
    "\n",
    "Let's start by creating a temporary view from a previous `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ec65a1e-f896-433a-a730-1085bebeead1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagecountsEnAllDF.createOrReplaceTempView(\"pagecounts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c967c7ed-1f0e-48bd-af60-bfd02f9ef273",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Now that we have a temporary view (or table) we can start expressing our queries and transformations in SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff4f4160-7992-46e3-b4d8-2670e6453434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM pagecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfbc2846-6ad9-4dcb-a7cc-7cb4c7c089ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM pagecounts\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1126db-cd0c-4d5f-be55-d69885012674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(\"pagecounts\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa8f4c59-6a86-4bfd-aa48-68f7ec9a83ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And we can just as easily express in SQL the distinct list of projects, and just because we can, we'll sort that list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3549e7f-3731-420f-bb63-ca37a26d0f93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT DISTINCT project FROM pagecounts ORDER BY project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc285be2-03aa-47a5-8a79-375720eac459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And converting from SQL back to a `DataFrame` is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd140676-6777-4d25-a964-88add77f76d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tableDF = spark.sql(\"SELECT DISTINCT project FROM pagecounts ORDER BY project\")\n",
    "tableDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c81d08cd-7793-4189-86cb-5b417830c234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "val scalaDF = spark.sql(\"SELECT * FROM pagecounts\")\n",
    "\n",
    "// Filter for a specific project and count the number of records\n",
    "val count = scalaDF.filter($\"project\" === \"en\").count()\n",
    "\n",
    "println(s\"Number of records for project 'en': $count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78035f9-4f87-4dc9-aacd-cd895f45677a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lab: Find Distinct Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e8c7aba-7e9f-4590-ba8d-03de4258370c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Replace <<FILL_IN>> with your code. \n",
    "\n",
    "df = (spark                    # Our SparkSession & Entry Point\n",
    "  .read                        # Our DataFrameReader\n",
    "  <<FILL_IN>>                  # Read in the parquet files\n",
    "  <<FILL_IN>>                  # Reduce the columns to just the one\n",
    "  <<FILL_IN>>                  # Produce a unique set of values\n",
    ")\n",
    "totalArticles = df.<<FILL_IN>> # Identify the total number of records remaining.\n",
    "\n",
    "print(\"Distinct Articles: {0:,}\".format(totalArticles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "250cfbe7-8f0d-4cca-af76-57c085c7b4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verify Your Work\n",
    "Run the following cell to verify that your `DataFrame` was created properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea6a9f5-2e2d-4f2d-ab6b-e7a4d9992cce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "expected = 1783138\n",
    "assert totalArticles == expected, \"Expected the total to be \" + str(expected) + \" but found \" + str(totalArticles)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4864718931953131,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "(Clone) 1-Describe-a-dataframe",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
