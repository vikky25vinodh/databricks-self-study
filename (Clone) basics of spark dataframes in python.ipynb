{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d409d08-0353-4660-89eb-5d8415abca58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d883f96f-5896-4e4e-990d-8e0714ace07e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7db76d5-b07e-4a9e-9fd7-a543d8b890b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Create a dataframe from list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d9b3b3-2767-4389-b390-943db1335625",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a Spark DataFrame from a list of tuples\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "#create a temporary view of the dataframe\n",
    "df.createOrReplaceTempView(\"temp_table\")\n",
    "display(spark.sql(\"select * from temp_table\"))\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n",
    "\n",
    "# Show the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Select and display the \"Name\" column\n",
    "display(df.select(\"Name\"))\n",
    "\n",
    "# Filter and display rows where age is greater than 30\n",
    "display(df.filter(df.Age > 30))\n",
    "\n",
    "# Group by \"Age\" and count the number of occurrences\n",
    "display(df.groupBy(\"Age\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4fb4816-971f-4690-a8e0-b1b3e964d16d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###some other ways to  create dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9fa1889-b70d-43cd-a720-0ba1a9299050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from a pandas dataframe\n",
    "import pandas as pd\n",
    "data = {'Name': ['Alice', 'Bob', 'Cathy'],\n",
    "        'Age': [34, 45, 29]}\n",
    "pdf = pd.DataFrame(data)\n",
    "display(pdf)\n",
    "# Create a Spark DataFrame from a pandas DataFrame\n",
    "spark_df = spark.createDataFrame(pdf)\n",
    "display(spark_df)\n",
    "\n",
    "#from a CSV file\n",
    "df=spark.read.csv(\"path of the csv file\", header=True, inferSchema=True)\n",
    "\n",
    "#from a JSON file\n",
    "df=spark.read.json(\"path of the json file\")\n",
    "\n",
    "#from a parquet file\n",
    "df=spark.read.parquet(\"path of the parquet file\")\n",
    "\n",
    "#from a text file\n",
    "df=spark.read.text(\"path of the text file\")\n",
    "\n",
    "#from an existing hive table\n",
    "df=spark.sql(\"select * from table_name\")\n",
    "# Create a temporary view from the DataFrame\n",
    "df.createOrReplaceTempView(\"my_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc6bd76d-026e-431b-b2df-632854354346",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##some aggregate operations in both as functions and as expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d546fdb7-d4c6-4097-b590-1848a410b833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate and display the average age\n",
    "from pyspark.sql import functions as F\n",
    "# Calculate and display the average age\n",
    "df.agg(F.avg('Age').alias('Average_Age')).show()\n",
    "\n",
    "# Calculate and display the average age\n",
    "df.selectExpr('avg(Age) as Average_Age').show()\n",
    "\n",
    "# Calculate and display the sum of ages\n",
    "df.agg(F.sum('Age').alias('Total_Age')).show()\n",
    "\n",
    "# Calculate and display the sum of ages\n",
    "df.selectExpr('sum(Age) as Total_Age').show()\n",
    "\n",
    "# Calculate and display the minimum age\n",
    "df.agg(F.min('Age').alias('Min_Age')).show()\n",
    "\n",
    "# Calculate and display the minimum age\n",
    "df.selectExpr('min(Age)as min_age').show()\n",
    "\n",
    "# Calculate and display the maximum age\n",
    "df.agg(F.max('Age').alias('Max_Age')).show()\n",
    "\n",
    "# Calculate and display the maximum age\n",
    "df.selectExpr('max(Age) as max_age').show()\n",
    "\n",
    "# Calculate and display the standard deviation of ages\n",
    "df.agg(F.stddev('Age').alias('Stddev_Age')).show()\n",
    "\n",
    "# Calculate and display the standard deviation of ages\n",
    "df.selectExpr('stddev(Age) as stddev_age').show()\n",
    "\n",
    "# Calculate and display the variance of ages\n",
    "df.agg(F.variance('Age').alias('Variance_Age')).show()\n",
    "\n",
    "# Calculate and display the variance of ages\n",
    "df.selectExpr('variance(Age) as variance_age').show()\n",
    "\n",
    "# Calculate and display the count of rows\n",
    "df.agg(F.count('Age').alias('Count_Age')).show()\n",
    "\n",
    "# Calculate and display the count of rows\n",
    "df.selectExpr('count(Age) as age_count').show()\n",
    "\n",
    "# Calculate and display the approximate count distinct of ages\n",
    "df.agg(F.approx_count_distinct('Age').alias('Approx_Count_Distinct_Age')).show()\n",
    "\n",
    "# Calculate and display the first age\n",
    "df.agg(F.first('Age').alias('First_Age')).show()\n",
    "\n",
    "# Calculate and display the first age\n",
    "df.selectExpr('first(Age) as first_age').show()\n",
    "\n",
    "# Calculate and display the last age\n",
    "df.agg(F.last('Age').alias('Last_Age')).show()\n",
    "\n",
    "# Calculate and display the last age\n",
    "df.selectExpr('last(Age) as last_age').show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f98016a-001b-4f55-bb1a-8c0b091e7d60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###few more operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "729d385b-af32-47a2-84e9-3798fb84bc77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "# Add a new column \"Senior\" based on the condition if age is greater than 40\n",
    "df = df.withColumn(\"Senior\", when(df.Age > 40,\"yes\").otherwise(\"no\"))\n",
    "display(df)\n",
    "\n",
    "# Sort the DataFrame by \"Age\" in descending order\n",
    "df_sorted = df.orderBy(df.Age.desc())\n",
    "display(df_sorted)\n",
    "\n",
    "# Drop the \"Senior\" column\n",
    "df = df.drop(\"Senior\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d3311f-e9b1-4bb1-8285-5c1303dca5a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##some file handling commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "209d4957-6b18-4483-adfa-8c1fda71ab8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create (touch) a new empty file\n",
    "dbutils.fs.put(\"/file path/\", \"\", True)\n",
    "\n",
    "# Write text to a file\n",
    "dbutils.fs.put(\"/file path/hello.txt\", \"Hello, Databricks!\", True)\n",
    "\n",
    "# List files in a directory\n",
    "display(dbutils.fs.ls(\"/directorypath/\"))\n",
    "\n",
    "# Read the contents of a file\n",
    "print(dbutils.fs.head(\"/file path/hello.txt\"))\n",
    "\n",
    "# Move (rename) a file\n",
    "dbutils.fs.mv(\"file:/tmp/hello.txt\", \"file:/tmp/greeting.txt\")\n",
    "\n",
    "# Copy a file\n",
    "dbutils.fs.cp(\"file:/tmp/greeting.txt\", \"file:/tmp/greeting_copy.txt\")\n",
    "\n",
    "# Remove (delete) a file\n",
    "dbutils.fs.rm(\"file:/tmp/example.txt\")\n",
    "\n",
    "# Remove a directory and its contents recursively\n",
    "dbutils.fs.rm(\"file:/tmp/\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11104ad8-0c1d-4201-bd17-7b5faebee9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##some basic shell commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6450cf67-9f00-441c-a9b3-ae9769382f45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all available magic commands\n",
    "#%lsmagic\n",
    "\n",
    "# Display the current working directory\n",
    "#%pwd\n",
    "\n",
    "# List files in the current directory\n",
    "#%ls\n",
    "\n",
    "# Measure the execution time of a Python code snippet\n",
    "#%timeit df.filter(df.Age > 30).count()\n",
    "\n",
    "# Run a shell command\n",
    "#!echo \"Hello, Databricks!\"\n",
    "\n",
    "# Display the history of commands executed\n",
    "#%history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92573995-b7c6-4c58-9741-aa26df488c66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%fs ls dbfs:/databricks-datasets/COVID/"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "0a424e8a-8d03-4413-8cd6-717274e6b8e4",
     "origId": 2355960514891735,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7263529692542297,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) basics of spark dataframes in python",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
