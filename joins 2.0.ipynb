{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2797347-b715-4c30-ae9d-93891e5d0c98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Joins in Spark DataFrames\n",
    "df1 = spark.createDataFrame([(1, \"A\"), (2, \"B\")], [\"id\", \"val1\"])\n",
    "df2 = spark.createDataFrame([(1, \"X\"), (3, \"Y\")], [\"id\", \"val2\"])\n",
    "\n",
    "# Inner Join\n",
    "inner_join = df1.join(df2, on=\"id\", how=\"inner\")\n",
    "display(inner_join)\n",
    "\n",
    "# Left Outer Join\n",
    "left_join = df1.join(df2, on=\"id\", how=\"left\")\n",
    "display(left_join)\n",
    "\n",
    "# Right Outer Join\n",
    "right_join = df1.join(df2, on=\"id\", how=\"right\")\n",
    "display(right_join)\n",
    "\n",
    "# Full Outer Join\n",
    "full_join = df1.join(df2, on=\"id\", how=\"outer\")\n",
    "display(full_join)\n",
    "\n",
    "# Left Semi Join\n",
    "semi_join = df1.join(df2, on=\"id\", how=\"left_semi\")\n",
    "display(semi_join)\n",
    "\n",
    "# Left Anti Join\n",
    "anti_join = df1.join(df2, on=\"id\", how=\"left_anti\")\n",
    "display(anti_join)\n",
    "\n",
    "# Cross Join\n",
    "cross_join = df1.crossJoin(df2)\n",
    "display(cross_join)\n",
    "\n",
    "# Union Operations\n",
    "df3 = spark.createDataFrame([(3, \"C\")], [\"id\", \"val1\"])\n",
    "union_df = df1.union(df3)\n",
    "display(union_df)\n",
    "\n",
    "# Union by Name\n",
    "df4 = spark.createDataFrame([(\"D\", 4)], [\"val1\", \"id\"])\n",
    "union_by_name_df = df1.unionByName(df4)\n",
    "display(union_by_name_df)\n",
    "\n",
    "# SQL Operations\n",
    "df1.createOrReplaceTempView(\"table1\")\n",
    "df2.createOrReplaceTempView(\"table2\")\n",
    "\n",
    "# SELECT, WHERE, GROUP BY, ORDER BY, LIMIT\n",
    "result = spark.sql(\"\"\"\n",
    "SELECT t1.id, t1.val1, t2.val2\n",
    "FROM table1 t1\n",
    "LEFT JOIN table2 t2 ON t1.id = t2.id\n",
    "WHERE t1.id > 0\n",
    "GROUP BY t1.id, t1.val1, t2.val2\n",
    "ORDER BY t1.id DESC\n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "display(result)\n",
    "\n",
    "# DISTINCT\n",
    "distinct_df = df1.select(\"val1\").distinct()\n",
    "display(distinct_df)\n",
    "\n",
    "# Aggregations\n",
    "agg_df = df1.groupBy(\"val1\").count()\n",
    "display(agg_df)\n",
    "\n",
    "# Subqueries\n",
    "subquery_df = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM (\n",
    "  SELECT id, val1 FROM table1 WHERE id > 1\n",
    ") sub\n",
    "\"\"\")\n",
    "display(subquery_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "joins 2.0",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
